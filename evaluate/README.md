
Evaluation
====================

Tools for evaluating the generated captions, consisting in large parts of the COCO caption evaluation code.

#Preparation

For all evaluation scripts, the output data must be in JSON format. To produce data in the correct format from the raw output data of our models, run the python script makeOutput.py . It creates a new file in the right format in the same folder as the original output file.

Arguments for makeOutput.py:
Argument 1: the raw output file to be formatted
Argument 2: the image id file
Argument 3: dev or test

Example:
python makeOutput.py ../train_data/1.4_5_6/coco_a_model/model_5_500/output_test ../train_data/1.4_5_6/coco_a_input/image_id.txt test


Additional interface for human evaluation 
===================
The purpose is to provide an interface where humans can evaluate the captions generated by a model.

The model is evaluated for 
1. Informativeness: Does the utterance provide all the useful information from the meaning representation?
2. Naturalness: Could the utterance have been produced by a native speaker?
3. Quality: How do you judge the overall quality of the utterance in terms of its grammatical correctness
and fluency?

The main file is humanEvalSession.py . 

Arguments for humanEvalSession.py:

Argument 1: path to the directory with results to be evaluated
Argument 2: test or dev 

Example: 
python3 humanEvalSession.py ../train_data/1.4_5_6/coco_a_model/model_5_500 test

The program will then ask for your name.
If this is the first time you are evaluating this output, enter any name (and memorize it).
If you are resuming an evaluation session for this data, enter the same name that you chose the first time.
The current state of your evaluation is saved in a log-file that will appear in this folder.

To exit a session just stop the program with CTRL-C. Data is saved regularly and you can resume the session where you stopped when running humanEvalSession.py for the next time and using the same 'username'.


Microsoft COCO Caption Evaluation
===================

Evaluation codes for MS COCO caption generation.

## Requirements ##
- java 1.8.0
- python 2.7

## Files ##

- evaluate.py: the main program that computes scores for a given result file
./pycocoevalcap: The folder where all evaluation codes are stored.
- evals.py: The file includes COCOEavlCap class that can be used to evaluate results on COCO.
- tokenizer: Python wrapper of Stanford CoreNLP PTBTokenizer
- bleu: Bleu evalutation codes
- meteor: Meteor evaluation codes
- rouge: Rouge-L evaluation codes
- cider: CIDEr evaluation codes
- spice: SPICE evaluation codes

## Setup ##

- You will first need to download the [Stanford CoreNLP 3.6.0](http://stanfordnlp.github.io/CoreNLP/index.html) code and models for use by SPICE. To do this, run:
    ./get_stanford_models.sh
- Note: SPICE will try to create a cache of parsed sentences in ./pycocoevalcap/spice/cache/. This dramatically speeds up repeated evaluations. The cache directory can be moved by setting 'CACHE_DIR' in ./pycocoevalcap/spice. In the same file, caching can be turned off by removing the '-cache' argument to 'spice_cmd'. 

## Run ##

To get the scores for your model's output, run

python2 evaluate.py arg1 arg2 arg3

Arguments:
1 path to the model output dir to be evaluated
2 dev or test
3 path to annotation file

python2 evaluate.py ../train_data/1.1_2_3/ms_coco_model/model_1 test ./annotations/captions_merged2014.json

It will print the results for the metrics as well as an average for the human evaluation scores to stdout.
## References ##

- [Microsoft COCO Captions: Data Collection and Evaluation Server](http://arxiv.org/abs/1504.00325)
- PTBTokenizer: We use the [Stanford Tokenizer](http://nlp.stanford.edu/software/tokenizer.shtml) which is included in [Stanford CoreNLP 3.4.1](http://nlp.stanford.edu/software/corenlp.shtml).
- BLEU: [BLEU: a Method for Automatic Evaluation of Machine Translation](http://www.aclweb.org/anthology/P02-1040.pdf)
- Meteor: [Project page](http://www.cs.cmu.edu/~alavie/METEOR/) with related publications. We use the latest version (1.5) of the [Code](https://github.com/mjdenkowski/meteor). Changes have been made to the source code to properly aggreate the statistics for the entire corpus.
- Rouge-L: [ROUGE: A Package for Automatic Evaluation of Summaries](http://anthology.aclweb.org/W/W04/W04-1013.pdf)
- CIDEr: [CIDEr: Consensus-based Image Description Evaluation](http://arxiv.org/pdf/1411.5726.pdf)
- SPICE: [SPICE: Semantic Propositional Image Caption Evaluation](https://arxiv.org/abs/1607.08822)

## Developers of the original MS COCO evaluation tools##
- Xinlei Chen (CMU)
- Hao Fang (University of Washington)
- Tsung-Yi Lin (Cornell)
- Ramakrishna Vedantam (Virgina Tech)
